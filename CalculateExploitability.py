import numpy as np
import itertools


class Exploitability:

    def __init__(self):
        self.expected_game_value = 0
        self.n_cards = 3
        self.action_dict = {0: 'p', 1: 'b'}

        # Pre-compute average strategies with CFR
        self.average_strategies = {'0 ': [0.75, 0.25], "0 pb":  [1.0, 0],
                                   "1 ": [1.0, 0.0], "1 pb":  [0.43, 0.57],
                                   "2 ": [0.25, 0.75], "2 pb": [0.0, 1.0],

                                   "0 b": [1.0, 0.0], "0 p": [0.66, 0.33],
                                   "1 b": [0.66, 0.33], "1 p": [1.0, 0.0],
                                   "2 b": [0.0, 1.0], "2 p": [0.0, 1.0]
                                   }
        self.best_response_strategies = dict()

        self.current_player = 0
        self.deck = np.array([0, 1, 2])
        self.n_actions = 2

        self.current_player = 0
        self.best_response_player = 0

    def compute_exploitability(self):
        kuhn_cards = [0, 1, 2]
        exploitability = 0
        self.best_response_player = 0

        # Walk tree and sum up best responses at each info set
        for mixed_deck in itertools.permutations(kuhn_cards):
            self.deck = mixed_deck
            self.best_response_player = 0
            self.best_response('', 1.0)
            self.best_response_player = 1
            self.best_response('', 1.0)

        # Set best response strategy, highest value = 1; all else = 0
        for key in self.best_response_strategies:
            best_response_sum = self.best_response_strategies[key]
            best_response = np.zeros(len(best_response_sum))
            best_response[np.argmax(best_response_sum)] = 1
            self.best_response_strategies[key] = best_response

        # Calculate the expected value of playing pre computed average strategy against the best response
        for mixed_deck in itertools.permutations(kuhn_cards):
            self.deck = mixed_deck
            ev_1 = self.calculate_expected_value(self.average_strategies, self.best_response_strategies, '')
            ev_2 = self.calculate_expected_value(self.best_response_strategies, self.average_strategies, '')
            exploitability += 1 / 6 * (ev_1 - ev_2)

        return exploitability

    # Walk tree, calc best response
    def best_response(self, history, reach_prob):
        n = len(history)
        player = n % 2
        player_card = self.deck[0] if player == 0 else self.deck[1]
        key = str(player_card) + " " + history
        if self.is_terminal(history):
            card_opponent = self.deck[1] if player == 0 else self.deck[0]
            reward = self.get_reward(history, player_card, card_opponent)
            return -reward

        action_utils = np.zeros(self.n_actions)
        if player == self.best_response_player:
            br_node = self.get_br_strategy_sum(key)
            best_value = 0
            for act in range(self.n_actions):
                next_history = history + self.action_dict[act]
                action_utils[act] = self.best_response(next_history, reach_prob)
                if action_utils[act] >= best_value:
                    best_value = action_utils[act]
            self.best_response_strategies[key] = br_node + (reach_prob * action_utils)
            return -best_value

        else:
            strategy = self.average_strategies[key]
            for a in range(self.n_actions):
                next_history = history + self.action_dict[a]
                action_utils[a] = -1 * self.best_response(next_history, reach_prob * strategy[a])
            util = np.sum(action_utils * strategy)
            return util

    def calculate_expected_value(self, p1_strategy, p2_strategy, history):
        n = len(history)
        player = n % 2
        player_card = self.deck[0] if player == 0 else self.deck[1]
        if self.is_terminal(history):
            card_opponent = self.deck[1] if player == 0 else self.deck[0]
            return -self.get_reward(history, player_card, card_opponent)

        key = str(player_card) + " " + history
        strategy = p1_strategy[key] if player == 0 else p2_strategy[key]
        action_utils = np.zeros(self.n_actions)
        for a in range(self.n_actions):
            next_history = history + self.action_dict[a]
            action_utils[a] = -1 * self.calculate_expected_value(p1_strategy, p2_strategy, next_history)
        return sum(action_utils * strategy)

    @staticmethod
    def is_terminal(history):
        if history[-2:] == 'pp' or history[-2:] == "bb" or history[-2:] == 'bp':
            return True

    @staticmethod
    def get_reward(history, player_card, opponent_card):
        terminal_pass = history[-1] == 'p'
        double_bet = history[-2:] == "bb"
        if terminal_pass:
            if history[-2:] == 'pp':
                return 1 if player_card > opponent_card else -1
            else:
                return 1
        elif double_bet:
            return 2 if player_card > opponent_card else -2

    def get_br_strategy_sum(self, key):
        if key not in self.best_response_strategies:
            self.best_response_strategies[key] = np.array([0.0, 0.0])
        return self.best_response_strategies[key]


if __name__ == "__main__":
    compute_exploitability = Exploitability()
    exploitability = compute_exploitability.compute_exploitability()
    print(exploitability)
